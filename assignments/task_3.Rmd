---
title: "Task 3: Modeling"
output: html_notebook
---

```{r}
pacman::p_load(tidyverse, tidytext, here)
```

## Bigram Model

```{r load_data}
sample_data <- read_csv(here("builds/B01_Sample_Dataset/sample_dataset.csv"),
                        col_types = "cic")
bad_words <- tibble(word = read_lines(here("data/D02_bad_words/bad_words.txt")))
```

```{r}
unigrams <- sample_data %>%
   unnest_tokens(word, value) %>%
   anti_join(bad_words) %>%
   filter(!grepl("[[:digit:]]|(^| )[[:punct::]]( |$)|_", word)) %>%
   count(word, sort = T) %>%
   rename(n_uni = n)

unigrams
```

```{r}
bigrams <- sample_data %>% 
   unnest_tokens(bigram, value, token = "ngrams", n = 2) %>%
   filter(!is.na(bigram)) %>%
   anti_join(bad_words, by = c("bigram" = "word")) %>%
   separate(bigram, c("word1", "word2"), sep = " ", remove = F) %>%
   filter(!(word1 %in% bad_words$word) & !(word2 %in% bad_words$word)) %>%
   filter(!grepl("[[:digit:]]|(^| )[[:punct::]]( |$)|_", bigram)) %>%
   count(bigram, word1, word2, sort = T) %>%
   rename(n_bi = n)

bigrams
```

```{r}
bigram_model <- bigrams %>%
   left_join(unigrams, by = c("word1" = "word")) %>%
   mutate(prob = n_bi / n_uni) %>%
   arrange(word1, desc(prob)) %>%
   select(word1, word2, prob)

bigram_model
```

```{r}
bigram_model %>%
   filter(word2 == "pizza")
```

```{r}
bigram_model %>%
   filter(word1 == "where")
```

```{r}
p_is_where <- bigram_model %>%
   filter(word1 == "where", word2 == "is") %>%
   pull(prob)
p_my_is <- bigram_model %>%
   filter(word1 == "is", word2 == "my") %>%
   pull(prob)
p_pizza_my <- bigram_model %>%
   filter(word1 == "my", word2 == "pizza") %>%
   pull(prob)
log(p_is_where) + log(p_my_is) 
```

## Perplexity

```{r}
N_uni <- nrow(unigrams)

P_uni <- unigrams %>%
   mutate(prob = log(n_uni / sum(.$n_uni))) %>%
   pull(prob) %>%
   sum()

P_uni^-(1/N_uni)
```

```{r}
N_bi <- bigram_model %>%
   filter(!is.na(prob)) %>% 
   nrow()

P_bi <- bigram_model %>%
   filter(!is.na(prob)) %>% 
   pull(prob) %>%
   log() %>%
   sum()

P_bi^-(1/N_bi)
```


```{r}
trigrams <- sample_data %>% 
   unnest_tokens(trigram, value, token = "ngrams", n = 3) %>%
   filter(!is.na(trigram)) %>%
   anti_join(bad_words, by = c("trigram" = "word")) %>%
   separate(trigram, c("word1", "word2", "word3"), sep = " ", remove = F) %>%
   filter(!(word1 %in% bad_words$word) & !(word2 %in% bad_words$word) & !(word3 %in% bad_words$word)) %>%
   filter(!grepl("[[:digit:]]|(^| )[[:punct::]]( |$)|_", trigram)) %>%
   count(trigram, word1, word2, word3, sort = T) %>%
   rename(n_tri = n)

trigrams %>%
   group_by(word1, word2) %>%
   arrange(word1, word2)
```

```{r}
input <- c("a", "bad")

trigrams %>%
   filter(word1 == input[1], word2 == input[2])
```

Probability
```{r}
bigrams %>%
   filter(word1 == input[1], word2 == input[2])
```

```{r}
count_input <- trigrams %>%
   filter(word1 == input[1], word2 == input[2]) %>%
   pull(n_tri) %>%
   sum()

trigrams %>%
   filter(word1 == input[1], word2 == input[2]) %>%
   mutate(prob = n_tri / count_input)
   
```

```{r}
prob_table <- trigrams %>%
   group_by(word1, word2) %>%
   mutate(n_bi = sum(n_tri)) %>% 
   ungroup() %>%
   group_by(word1) %>%
   mutate(n_uni = sum(n_tri)) %>%
   ungroup() %>%
   mutate(prob3 = n_tri / n_bi,
          prob2 = n_bi / n_uni,
          prob1 = n_uni / sum(n_uni)) %>%
   arrange(word1, word2)
```

```{r}
input2 <- c("a", "lot")

prob_table %>%
   filter(word1 == input2[1], word2 == input2[2]) %>%
   arrange(desc(prob3))
```

```{r}
input3 <- c("a", "help")

prob_table %>%
   filter(word1 == input[1], word2 == input3[2])
```

```{r}
prob_table %>%
   filter(word1 == input3[2]) %>%
   group_by(word1, word2) %>%
   summarize(prob2 = 0.4*unique(prob2)) %>%
   arrange(desc(prob2))
```

```{r}
sum(prob_table$word1 == input2[1] & prob_table$word2 == input2[2])
```


```{r}
model_1 <- function(input1, input2) {
   #Check for matching trigrams
   if(sum(prob_table$word1 == input1 & prob_table$word2 == input2) > 0) {
      results <- prob_table %>%
         filter(word1 == input1, word2 == input2) %>%
         select(suggestion = word3, prob = prob3) %>%
         arrange(desc(prob))
   } else if(sum(prob_table$word1 == input2) > 0) {
      results <- prob_table %>%
         filter(word1 == input2) %>%
         group_by(word2) %>%
         summarize(prob = unique(prob2)) %>%
         select(suggestion = word2, prob) %>%
         arrange(desc(prob))
   } else {
      results <- prob_table %>%
         group_by(word1) %>%
         summarize(prob = unique(prob1)) %>%
         select(suggestion = word1, prob) %>%
         arrange(desc(prob)) %>%
         filter(1:3)
   }
   return(results)
}
```

```{r}
model_1("food", "to")
```

```{r}
model_1("help", "me")
```

```{r}
model_1("hi", "how")
```

```{r}
model_1("what", "are")
```

```{r}
model_1("are", "you")
```

```{r}
model_1("you", "doing")
```

```{r}
model_1("doing", "later")
```



## Stupid Backoff

# Approach?

1. Create clean tables of unigram, bigrams, & trigrams

   - Remove profanity
   - Remove digits
   - Remove special characters
   
2. Calculate probabilites for tri, bi, & uni
3. Build function to return top 3 recommended words